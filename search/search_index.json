{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Benchmarking simulation-based inference Recent advances have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark for such 'likelihood-free' algorithms has been lacking: We set out to fill this gap, carefully select tasks and metrics, and evaluate several canonical algorithms. Through this website you can explore all results of our manuscript , including comparisons on all metrics and plotting of posteriors for each of more than 10 000 runs. Keep reading for a brief summary of the paper, or jump right into interactive results through the navigation. We provide a framework to benchmark your own algorithms, full code and results for reproducibility, and invite contributions. Our motivation and methods Open benchmarks can be an important component of transparent and reproducible computational research. Surprisingly, a benchmark framework for simulation-based inference (SBI) has been lacking, possibly due to the challenging endeavour of designing benchmarking tasks and defining suitable performance metrics. We selected a set of initial algorithms representing four distinct approaches to SBI, analyzed multiple performance metrics which have been used in the literature and implemented ten tasks including ones popular in the field. Overview of algorithms. Classification and schemes following Cranmer et al. (2020) . Algorithms . We compare algorithms belonging to four distinct approaches to SBI: Classical ABC approaches as well as model-based approaches approximating likelihoods, posteriors, or density ratios. We contrast algorithms that use the prior distribution to propose parameters against ones that sequentially adapt the proposal. Keeping our initial selection of algorithms focused allowed us to carefully consider implementation details and hyperparameters. Metrics. The shortcomings of commonly used metrics (see paper for details) led us to focus on tasks for which a likelihood can be evaluated, which allowed us to calculate reference (\u2018ground-truth\u2019) posteriors. These reference posteriors are made available to allow rapid evaluation of SBI algorithms. While we compare algorithms in terms of classifier 2-sample tests (C2ST) in the paper, the website allows comparisons in terms of all metrics we considered. Tasks. We focused on eight purely statistical problems and two problems relevant in applied domains, with diverse dimensionalities of parameters and data. Key findings The full potential of the benchmark will be realized when it is populated with additional community-contributed algorithms and tasks. However, our initial version already provides useful insights, we find that: the choice of performance metric is critical (commonly used metrics such as the likelihood of true parameters or MMD with median heuristic have important shortcomings); the performance of the algorithms on some tasks leaves substantial room for improvement; sequential estimation generally improves sample efficiency; for small and moderate simulation budgets, neural-network based approaches outperform classical ABC algorithms, confirming recent progress in the field; but that there is no algorithm to rule them all. The performance ranking of algorithms is task-dependent, pointing to a need for better guidance or automated procedures for choosing which algorithm to use when. In the manuscript, we included some considerations and recommendations for practioners, based on our current results and understanding, and dedicated a page to discussing various limitations. Find out more in our manuscript: arXiv.org/abs/2021.01xxxxx How can I benchmark my own algorithms? We believe that the full potential of the benchmark will be revealed as more researchers participate and contribute. In order to facilitate this process, we provide sbibm , a SBI benchmark framework designed to be highly extensible and easily used. See Code & Reproducibility for explanations and demos.","title":"Overview"},{"location":"#benchmarking-simulation-based-inference","text":"Recent advances have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark for such 'likelihood-free' algorithms has been lacking: We set out to fill this gap, carefully select tasks and metrics, and evaluate several canonical algorithms. Through this website you can explore all results of our manuscript , including comparisons on all metrics and plotting of posteriors for each of more than 10 000 runs. Keep reading for a brief summary of the paper, or jump right into interactive results through the navigation. We provide a framework to benchmark your own algorithms, full code and results for reproducibility, and invite contributions.","title":"Benchmarking simulation-based inference"},{"location":"#our-motivation-and-methods","text":"Open benchmarks can be an important component of transparent and reproducible computational research. Surprisingly, a benchmark framework for simulation-based inference (SBI) has been lacking, possibly due to the challenging endeavour of designing benchmarking tasks and defining suitable performance metrics. We selected a set of initial algorithms representing four distinct approaches to SBI, analyzed multiple performance metrics which have been used in the literature and implemented ten tasks including ones popular in the field. Overview of algorithms. Classification and schemes following Cranmer et al. (2020) . Algorithms . We compare algorithms belonging to four distinct approaches to SBI: Classical ABC approaches as well as model-based approaches approximating likelihoods, posteriors, or density ratios. We contrast algorithms that use the prior distribution to propose parameters against ones that sequentially adapt the proposal. Keeping our initial selection of algorithms focused allowed us to carefully consider implementation details and hyperparameters. Metrics. The shortcomings of commonly used metrics (see paper for details) led us to focus on tasks for which a likelihood can be evaluated, which allowed us to calculate reference (\u2018ground-truth\u2019) posteriors. These reference posteriors are made available to allow rapid evaluation of SBI algorithms. While we compare algorithms in terms of classifier 2-sample tests (C2ST) in the paper, the website allows comparisons in terms of all metrics we considered. Tasks. We focused on eight purely statistical problems and two problems relevant in applied domains, with diverse dimensionalities of parameters and data.","title":"Our motivation and methods"},{"location":"#key-findings","text":"The full potential of the benchmark will be realized when it is populated with additional community-contributed algorithms and tasks. However, our initial version already provides useful insights, we find that: the choice of performance metric is critical (commonly used metrics such as the likelihood of true parameters or MMD with median heuristic have important shortcomings); the performance of the algorithms on some tasks leaves substantial room for improvement; sequential estimation generally improves sample efficiency; for small and moderate simulation budgets, neural-network based approaches outperform classical ABC algorithms, confirming recent progress in the field; but that there is no algorithm to rule them all. The performance ranking of algorithms is task-dependent, pointing to a need for better guidance or automated procedures for choosing which algorithm to use when. In the manuscript, we included some considerations and recommendations for practioners, based on our current results and understanding, and dedicated a page to discussing various limitations. Find out more in our manuscript: arXiv.org/abs/2021.01xxxxx","title":"Key findings"},{"location":"#how-can-i-benchmark-my-own-algorithms","text":"We believe that the full potential of the benchmark will be revealed as more researchers participate and contribute. In order to facilitate this process, we provide sbibm , a SBI benchmark framework designed to be highly extensible and easily used. See Code & Reproducibility for explanations and demos.","title":"How can I benchmark my own algorithms?"},{"location":"code/","text":"Code and Reproducibility All details about sbibm can be found at: github.com/sbi-benchmark/sbibm Design of sbibm We designed sbibm to be easily used with any external code implementing SBI algorithms. Currently, we provide integrations with sbi , pyabc , pyabcranger , as well as an experimental integration with elfi . Quickstart Having installed the framework via pip install sbibm , using it is very simple, as we sketch out in the example code below: import sbibm task = sbibm.get_task(\"slcp\") # See sbibm.get_available_tasks() for all tasks prior = task.get_prior() simulator = task.get_simulator() observation = task.get_observation(num_observation=1) # 10 per task # Generating simulations by sampling from prior thetas = prior(num_samples=10_000) xs = simulator(thetas) # We could use the above objects to implement a custom inference algorithm, # or import one implement through the toolboxes mentioned above from sbibm.algorithms.sbi import rej_abc # See help(rej_abc) for keywords posterior_samples = rej_abc(task=\"slcp\", num_observation=1, num_simulations=100_000) # Compare to the reference posterior from sbibm.metrics import c2st reference_samples = task.get_reference_posterior(observation=1) c2st_accuracy = c2st(reference_samples, posterior_samples) # Visualise both posteriors from sbibm.visualise import fig_posterior fig_posterior(task=\"slcp\", observation=1, samples=[posterior_samples]) # Get results from other algorithms for comparison from sbibm.visualise import fig_metrics results_df = sbibm.get_results(task=\"slcp\") fig_metrics(results_df) Reproducing paper results All results of the paper and scripts that generated them using sbibm can be found at: github.com/sbi-benchmark/results/benchmarking_sbi","title":"Code & Reproducibility"},{"location":"code/#code-and-reproducibility","text":"All details about sbibm can be found at: github.com/sbi-benchmark/sbibm","title":"Code and Reproducibility"},{"location":"code/#design-of-sbibm","text":"We designed sbibm to be easily used with any external code implementing SBI algorithms. Currently, we provide integrations with sbi , pyabc , pyabcranger , as well as an experimental integration with elfi .","title":"Design of sbibm"},{"location":"code/#quickstart","text":"Having installed the framework via pip install sbibm , using it is very simple, as we sketch out in the example code below: import sbibm task = sbibm.get_task(\"slcp\") # See sbibm.get_available_tasks() for all tasks prior = task.get_prior() simulator = task.get_simulator() observation = task.get_observation(num_observation=1) # 10 per task # Generating simulations by sampling from prior thetas = prior(num_samples=10_000) xs = simulator(thetas) # We could use the above objects to implement a custom inference algorithm, # or import one implement through the toolboxes mentioned above from sbibm.algorithms.sbi import rej_abc # See help(rej_abc) for keywords posterior_samples = rej_abc(task=\"slcp\", num_observation=1, num_simulations=100_000) # Compare to the reference posterior from sbibm.metrics import c2st reference_samples = task.get_reference_posterior(observation=1) c2st_accuracy = c2st(reference_samples, posterior_samples) # Visualise both posteriors from sbibm.visualise import fig_posterior fig_posterior(task=\"slcp\", observation=1, samples=[posterior_samples]) # Get results from other algorithms for comparison from sbibm.visualise import fig_metrics results_df = sbibm.get_results(task=\"slcp\") fig_metrics(results_df)","title":"Quickstart"},{"location":"code/#reproducing-paper-results","text":"All results of the paper and scripts that generated them using sbibm can be found at: github.com/sbi-benchmark/results/benchmarking_sbi","title":"Reproducing paper results"},{"location":"contribute/","text":"Contribute The full potential of the benchmark will be realized when it is populated with additional community-contributed algorithms and tasks.","title":"Contributions"},{"location":"contribute/#contribute","text":"The full potential of the benchmark will be realized when it is populated with additional community-contributed algorithms and tasks.","title":"Contribute"},{"location":"results/correlations/","text":"","title":"Correlations"},{"location":"results/download/","text":"Dataframes ...","title":"Dataframes"},{"location":"results/download/#dataframes","text":"...","title":"Dataframes"},{"location":"results/hiplot/","text":"","title":"Hiplot"},{"location":"results/metrics/","text":"","title":"Metrics"},{"location":"results/offline/","text":"Offline The server hosting the interactive results is unfortunately offline at the moment. We hope it will be back online soon, for updates follow @janmatthis .","title":"Offline"},{"location":"results/offline/#offline","text":"The server hosting the interactive results is unfortunately offline at the moment. We hope it will be back online soon, for updates follow @janmatthis .","title":"Offline"},{"location":"results/posteriors/","text":"","title":"Posteriors"}]}